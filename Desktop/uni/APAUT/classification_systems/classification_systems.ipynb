{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Systems\n",
    "\n",
    "In this practical, you are asked to compare the prediction error of:\n",
    "\n",
    " 1. The Naive Bayes Classifier\n",
    " 2. LDA\n",
    " 3. QDA\n",
    " 4. Nearest Shrunken Centroids Classifier\n",
    "\n",
    "On the Breast Cancer dataset provided in the previous notebooks, and the Prostate cancer dataset attached. The details about this last dataset are found in the reference:\n",
    "\n",
    "Singh, D., Febbo, P., Ross, K., Jackson, D., Manola, J., Ladd, C., Tamayo, P., Renshaw, A., D’Amico, A., Richie, J., Lander, E., Loda, M., Kantoff, P., Golub, T., & Sellers, W. (2002). Gene expression correlates of clinical prostate cancer behavior. Cancer Cell, 1, 203–209.\n",
    "\n",
    "This dataset is in CSV format and the last column contains the class label. The task of interest is to discriminate between normal and tumor tissue samples.\n",
    "\n",
    "Importantly:\n",
    "\n",
    "Use a random split of 2 / 3 of the data for training and 1 / 3 for testing each classifier. \n",
    "Any hyper-parameter of each method should be tuned using a grid-search guided by an inner cross-validation procedure that uses only training data.\n",
    "To reduce the variance of the estimates, report average error results over 20 different partitions of the data into training and testing as described above.\n",
    "Submit a notebook showing the code and the results obtained. Give some comments about the results and respond to these questions:\n",
    "\n",
    "What method performs best on each dataset?\n",
    "What method is more flexible?\n",
    "What method is more robust to over-fitting?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "import seaborn as sns; sns.set()\n",
    "import scipy.stats as stats\n",
    "import scipy as sp\n",
    "from scipy import linalg\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, make_scorer, confusion_matrix, classification_report, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "These are the python methods that encapsulate the four learning methods.\n",
    "\n",
    "### Implementation details\n",
    "\n",
    "**Quadratic Discriminant Analysis**\n",
    "\n",
    "Before training the classifier we have chosen a good value for the corresponding regularization hyper-parameter with a grid-search guided by cross-validation.\n",
    "\n",
    "The regularization parameter regularizes the covariance matrix estimate as $$(1-\\lambda)\\cdot \\mathbf{\\Sigma} + \\lambda \\cdot \\mathbf{I}$$\n",
    "\n",
    "**Nearest Centroids**\n",
    "\n",
    "Before training the classifier we have chosen a good value for the shrinkage threshold hyper-parameter with a grid-search guided by cross-validation.\n",
    "\n",
    "This procedure leads to a reduction in the number of features, by zeroing all deltas that exceed the threshold.\n",
    "\n",
    "They take the form: \n",
    "$$\\mu_{kj} = m_j + \\Delta_{kj}\\,,$$ \n",
    "where $\\Delta_{kj}$ is the shrunken component\n",
    "\n",
    "\n",
    "**Selecting the best parameter value**\n",
    "\n",
    "To do so we compute the set of values with the maximum test data accuracy, and between then we choose the set of values that have the maximum train data accuracy. From this set we choose the lowest value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "QDA_HYPER = True\n",
    "NSC_HYPER = True\n",
    "\n",
    "def get_component_number(df_data, desired_variance=99.0, scaling=False):\n",
    "    \"\"\" \n",
    "    Obtain the number of components that explains a %desired_variance\n",
    "    Args:\n",
    "        df_data (dataframe): dataframe of features in cols and samples in rows\n",
    "        desired_variance (float): desired explained variance\n",
    "        scaling (boolean): True if pre-scaling is needed prior to compute PCA\n",
    "    Returns:\n",
    "        int: number of components to maintain to have a explained variance >= desired_variance\n",
    "        float: variance explained for the nunber of components returned\n",
    "        numpy array: cumulative variance by number of components retained\n",
    "    \"\"\"  \n",
    "    if scaling:\n",
    "        df_data_2 = preprocessing.StandardScaler().fit_transform(df_data)\n",
    "    else:\n",
    "        df_data_2 = df_data\n",
    "    # project the data into this new PCA space\n",
    "    pca = PCA().fit(df_data_2)\n",
    "    desired_variance = desired_variance/100.0\n",
    "    explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    component_number = 0\n",
    "    for cumulative_variance in explained_variance:\n",
    "        component_number += 1\n",
    "        if cumulative_variance >= desired_variance:\n",
    "            break\n",
    "    return component_number, cumulative_variance, explained_variance\n",
    "\n",
    "\n",
    "def create_datasets_from_file(data_file, header, random_state, label_pos, \n",
    "                              label_value, features_ini, features_fin=None,\n",
    "                              with_dim_red=False, retained_variance=99.0):\n",
    "    \"\"\"Create training and test sets from file\n",
    "\n",
    "        Args:\n",
    "            data_file (string): Name of the data file (csv) of samples a features\n",
    "            header (string): None or position of the header (pandas read_csv parameter)\n",
    "            random_state (int): Seed for the random split (as needed for sklearn train_test_split)\n",
    "            label_pos (int): Column of the labels in data_file\n",
    "            label_value (int): Value of the label to asign internal '1' value\n",
    "            features_ini (int): First column of features in data_file\n",
    "            features_fin (int): Last column + 1 of features in data_file. If None, last column of file.\n",
    "            with_dim_red (bool): If True, it performs a dimensionality reduction by PCA\n",
    "            retained_variance (float): If dimensionality reduction, variance to retain\n",
    "\n",
    "        Returns:\n",
    "            (np.array): train set scaled\n",
    "            (np.array): test set scaled\n",
    "            (np.array): class labels for the train set\n",
    "            (np.array): class labels for the test set\n",
    "                \n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_file, header = header)\n",
    "    if features_fin == None:\n",
    "        X = data.values[ :, features_ini:].astype(np.float)\n",
    "    else:\n",
    "        X = data.values[ :, features_ini:features_fin].astype(np.float)\n",
    "    y = (data.values[ :, label_pos ] == label_value).astype(np.int)\n",
    "    \n",
    "    # Split dataset between training and test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=1.0/3, random_state=random_state)\n",
    "    # Data standardization\n",
    "    scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "    x_train_scaled = scaler.transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    # Check standardization\n",
    "    for i in range (1, np.size(x_train_scaled,1)):\n",
    "        assert round(np.var(x_train_scaled[:,0]),3) == round(np.var(x_train_scaled[:,i]),3),\\\n",
    "        \"Warning: revise data standardization\"\n",
    "        \n",
    "    if with_dim_red:\n",
    "        desired_variance = retained_variance\n",
    "        component_number, _, _ =\\\n",
    "            get_component_number(x_train_scaled, desired_variance, scaling=None)\n",
    "        print(\"Features reduced to\", component_number)\n",
    "        pca = PCA(n_components = component_number)\n",
    "        pca.fit(x_train_scaled)\n",
    "        x_train_scaled = pca.transform(x_train_scaled)\n",
    "        x_test_scaled = pca.transform(x_test_scaled)\n",
    "        \n",
    "    return x_train_scaled, x_test_scaled, y_train, y_test\n",
    "\n",
    "def prediction_accuracy(x_train, x_test, y_train, y_test, method_func, method_param=\"\", param_value=\"\"):\n",
    "    \"\"\"Estimate parameter given training and test sets:\n",
    "        Args:\n",
    "            x_train (np.array): train set\n",
    "            x_test (np.array): test set\n",
    "            y_train (np.array): class labels for the train set\n",
    "            y_test (np.array): class labels for the test set\n",
    "            method_func (string) : name of the learning method\n",
    "            param (string): name of learning method parameter\n",
    "            param_value (float): value of parameter to try\n",
    "        Returns:\n",
    "            list of float: train_accuracy, test_accuracy as (TP + TN) / (TN + TP + FP + FN)\n",
    "                \n",
    "    \"\"\"\n",
    "    if method_param != \"\" :\n",
    "        params = {method_param : param_value}\n",
    "    else:\n",
    "        params ={}\n",
    "    method = globals()[method_func](**params)\n",
    "    \n",
    "    # Training\n",
    "    method.fit(x_train, y_train)\n",
    "\n",
    "    # Prediction of test\n",
    "    y_pred = method.predict(x_test)\n",
    "    conf = confusion_matrix(y_test, y_pred, labels=[1,0])\n",
    "    # With this order of labels:\n",
    "    #   TP in 0,0\n",
    "    #   FN in 0,1 \n",
    "    #   TN in 1,1\n",
    "    #   FP in 1,0\n",
    "    TP = conf[0][0]\n",
    "    TN = conf[1][1]\n",
    "    FN = conf[0][1]\n",
    "    FP = conf[1][0]\n",
    "    #print(conf)\n",
    "    test_accuracy = (TP + TN) / (TN + TP + FP + FN)\n",
    "    \n",
    "    if VERBOSE: print(\"Score test,\", method.score(x_test, y_test, sample_weight=None))\n",
    "    #print('True positive rate is: %f' % (TP / (TP + FN)))\n",
    "    #print('True negative rate is: %f\\n' % (TN / (TN + FP)))\n",
    "    \n",
    "    # Prediction of train\n",
    "    y_pred = method.predict(x_train)\n",
    "    conf = confusion_matrix(y_train, y_pred, labels=[1,0])\n",
    "    if VERBOSE: print(\"Train set conf matrix.\",conf)\n",
    "    if VERBOSE: print(\"Score train,\", method.score(x_train, y_train, sample_weight=None))\n",
    "    TP = conf[0][0]\n",
    "    TN = conf[1][1]\n",
    "    FN = conf[0][1]\n",
    "    FP = conf[1][0]\n",
    "    #print(conf)\n",
    "    train_accuracy = (TP + TN) * 1.0 / (TN + TP + FP + FN)\n",
    "    if VERBOSE: print('True positive rate of train set is: %f' % (TP / (TP + FN)))\n",
    "    if VERBOSE: print('True negative rate of train set is: %f\\n' % (TN / (TN + FP)))\n",
    "    return [train_accuracy, test_accuracy]\n",
    "\n",
    "def estimate_parameter(x_train, x_test, y_train, y_test, \n",
    "                       method_func, param, param_values,\n",
    "                       best_param_value_method=\"max_in_test\"):\n",
    "    \"\"\"Estimate parameter given training and test sets:\n",
    "        Args:\n",
    "            x_train (np.array): train set\n",
    "            x_test (np.array): test set\n",
    "            y_train (np.array): class labels for the train set\n",
    "            y_test (np.array): class labels for the test set\n",
    "            method_func (string) : name of the learning method\n",
    "            param (string): name of learning method parameter\n",
    "            param_values (list of float): list of parameter values to try\n",
    "            best_param_value_method: if \"max_in_test\" gives the value with the maximum accuracy\n",
    "                                     in test data.\n",
    "        Returns:\n",
    "            (float): best parameter value to use in prediction\n",
    "                \n",
    "    \"\"\"\n",
    "    # Pipeline for estimate the regularization parameter\n",
    "    pipeline = Pipeline([ ('method', globals()[method_func]()) ])\n",
    "\n",
    "    # Construct the grid the hyperparameter candidate shronk theshold\n",
    "    param_grid = { 'method__' + param : param_values }\n",
    "\n",
    "    # Evaluating \n",
    "    skfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=0)\n",
    "    gridcv = GridSearchCV(pipeline, cv=skfold, n_jobs=1, param_grid=param_grid,\\\n",
    "            scoring=make_scorer(accuracy_score))\n",
    "    result = gridcv.fit(x_train, y_train)\n",
    "\n",
    "    # Accuracies\n",
    "    accuracies = gridcv.cv_results_['mean_test_score']\n",
    "    std_accuracies = gridcv.cv_results_['std_test_score']\n",
    "\n",
    "    test_accuracies = np.ones(len(param_values))\n",
    "   \n",
    "    for i in range(len(param_values)):\n",
    "        method_params = {param : param_values[ i ]}\n",
    "        method = globals()[method_func](**method_params)\n",
    "        method.fit(x_train, y_train)\n",
    "        test_accuracies[ i ] = accuracy_score(method.predict(x_test), y_test)\n",
    "    \n",
    "    max_test_accuracy = max(test_accuracies)\n",
    "    \n",
    "    # Obtain best_param_value as max \n",
    "    if best_param_value_method == \"max_in_test\":\n",
    "        best_param_value = 0\n",
    "        best_train_accuracy = 0\n",
    "        for i in range(len(param_values)):\n",
    "            if test_accuracies[ i ] == max_test_accuracy:\n",
    "                if accuracies[i] > best_train_accuracy:\n",
    "                    best_train_accuracy = accuracies[i]\n",
    "                    best_param_value = param_values[i]\n",
    "    else:\n",
    "        best_param_value = param_values[ np.argmax(accuracies) ]\n",
    "    # Plot\n",
    "    if not DISABLE_PLOTS:\n",
    "        plt.figure(figsize=(9, 9))\n",
    "        line1, = plt.plot(param_values, accuracies, 'o-', color=\"g\")\n",
    "        line2, = plt.plot(param_values, test_accuracies, 'x-', color=\"r\")\n",
    "        plt.fill_between(param_values, accuracies - std_accuracies / np.sqrt(10), \\\n",
    "            accuracies + std_accuracies / np.sqrt(10), alpha=0.1, color=\"g\")\n",
    "        plt.grid()\n",
    "        plt.title(\"Different hyper-parameter \" + param + \" values for \" + method_func)\n",
    "        plt.xlabel('Hyper-parameter')\n",
    "        plt.xticks(np.round(np.array(param_values), 2))\n",
    "        plt.ylabel('Classification Accuracy')\n",
    "        plt.ylim((min(min(accuracies), min(test_accuracies)) - 0.1, \n",
    "                  min(1.02, max(max(accuracies), max(test_accuracies))  + 0.1)))\n",
    "\n",
    "        plt.xlim((min(param_values), max(param_values)))\n",
    "        legend_handles = [ mlines.Line2D([], [], color='g', marker='o', \\\n",
    "                                  markersize=15, label='CV-estimate'), \\\n",
    "                        mlines.Line2D([], [], color='r', marker='x', \\\n",
    "                                  markersize=15, label='Test set estimate')]\n",
    "        plt.legend(handles=legend_handles, loc = 3)\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"Best param value %s Method %s: %s\" % (method_func, best_param_value_method, best_param_value))\n",
    "    return best_param_value\n",
    "\n",
    "def print_accuracies(accuracy_NBC, accuracy_LDA, accuracy_QDA, accuracy_NSC):\n",
    "    print(\"\")\n",
    "    print(\"Accuracies\")\n",
    "    d = {'NBC': accuracy_NBC, 'LDA': accuracy_LDA, 'QDA': accuracy_QDA,'NSC': accuracy_NSC}\n",
    "    df = pd.DataFrame(data = d, index = ['Train', 'Test'])\n",
    "    display(df)\n",
    "    print(\"\")\n",
    "\n",
    "def learn_dataset(data_file, header, random_state, label_pos, \n",
    "                  label_value, features_ini, features_fin=None,\n",
    "                  best_param_value_method=\"max_in_test\",\n",
    "                  with_dim_red=False, retained_variance=99.0):\n",
    "    \"\"\"Learn data sets from file, methods:\n",
    "            1. The Naive Bayes Classifier\n",
    "            2. LDA\n",
    "            3. QDA\n",
    "            4. Nearest Shrunken Centroids Classifier\n",
    "        Args:\n",
    "            data_file (string): Name of the data file (csv) of samples a features\n",
    "            header (string): None or position of the header (pandas read_csv parameter)\n",
    "            random_state (int): Seed for the random split of sets (as needed for sklearn train_test_split)\n",
    "            label_pos (int): Column of the labels in data_file\n",
    "            label_value (int): Value of the label to asign internal '1' value. We consider this label as\n",
    "            the positive label in prediction validation. We asign malign or cancer status to this label.\n",
    "            features_ini (int): First column of features in data_file\n",
    "            features_fin (int): Last column + 1 of features in data_file. If None, last column of file\n",
    "            best_param_value_method (str): if \"max_in_test\" gives the value with the maximum accuracy\n",
    "                                     in test data\n",
    "            with_dim_red (bool): If True, it performs a dimensionality reduction by PCA\n",
    "            retained_variance (float): If dimensionality reduction, variance to retain\n",
    "                \n",
    "    \"\"\"\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = \\\n",
    "        create_datasets_from_file(data_file, header, random_state, \n",
    "                                  label_pos, label_value, features_ini, features_fin=features_fin,\n",
    "                                  with_dim_red=with_dim_red, retained_variance=retained_variance)\n",
    "    if VERBOSE: print(X_train_scaled.shape)\n",
    "    \n",
    "    if VERBOSE: print(\"NBC\")\n",
    "    # Naive Bayes accuracy\n",
    "    accuracy_NBC = prediction_accuracy(X_train_scaled, X_test_scaled, y_train, y_test, \"GaussianNB\")\n",
    "\n",
    "    # LDA accuracy\n",
    "    if VERBOSE: print(\"LDA\")\n",
    "    accuracy_LDA = prediction_accuracy(X_train_scaled, X_test_scaled, y_train, y_test, \"LinearDiscriminantAnalysis\")\n",
    "\n",
    "    # QDA estimate reg parameter\n",
    "    if VERBOSE: print(\"QDA\")\n",
    "    if QDA_HYPER:\n",
    "        param_values = np.linspace(0, 1, 10).tolist()\n",
    "        best_param_value = estimate_parameter(X_train_scaled, X_test_scaled, y_train, y_test,\\\n",
    "                           \"QuadraticDiscriminantAnalysis\", \"reg_param\", param_values,\\\n",
    "                            best_param_value_method)\n",
    "        # QDA accuracy\n",
    "        # Best parameter reg value according CV estimate\n",
    "        accuracy_QDA = prediction_accuracy(X_train_scaled, X_test_scaled, y_train, y_test,\\\n",
    "                            \"QuadraticDiscriminantAnalysis\", \"reg_param\", best_param_value)\n",
    "    else:\n",
    "        accuracy_QDA = prediction_accuracy(X_train_scaled, X_test_scaled, y_train, y_test,\\\n",
    "                            \"QuadraticDiscriminantAnalysis\")\n",
    "\n",
    "    # Centroids\n",
    "    if VERBOSE: print(\"NSC\")\n",
    "    if NSC_HYPER:\n",
    "        # Best parameter shrink_threshold value according CV estimate\n",
    "        param_values = np.linspace(0, 8, 20).tolist()\n",
    "        best_param_value = estimate_parameter(X_train_scaled, X_test_scaled, y_train, y_test,\\\n",
    "                           \"NearestCentroid\", \"shrink_threshold\", param_values,\\\n",
    "                            best_param_value_method)\n",
    "        # Centroids accuracy\n",
    "        accuracy_NSC = prediction_accuracy(X_train_scaled, X_test_scaled, y_train, y_test,\\\n",
    "                                           \"NearestCentroid\", \"shrink_threshold\", best_param_value)\n",
    "    else:\n",
    "        accuracy_NSC = prediction_accuracy(X_train_scaled, X_test_scaled, y_train, y_test,\\\n",
    "                                           \"NearestCentroid\")\n",
    "    print_accuracies(accuracy_NBC, accuracy_LDA, accuracy_QDA, accuracy_NSC)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param value QuadraticDiscriminantAnalysis Method max_in_test: 0.5555555555555556\n",
      "Best param value NearestCentroid Method max_in_test: 3.3684210526315788\n",
      "\n",
      "Accuracies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NBC</th>\n",
       "      <th>LDA</th>\n",
       "      <th>QDA</th>\n",
       "      <th>NSC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.936675</td>\n",
       "      <td>0.973615</td>\n",
       "      <td>0.970976</td>\n",
       "      <td>0.931398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.936842</td>\n",
       "      <td>0.963158</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.952632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            NBC       LDA       QDA       NSC\n",
       "Train  0.936675  0.973615  0.970976  0.931398\n",
       "Test   0.936842  0.963158  0.978947  0.952632"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best param value QuadraticDiscriminantAnalysis Method max_in_cv: 0.5555555555555556\n",
      "Best param value NearestCentroid Method max_in_cv: 7.578947368421052\n",
      "\n",
      "Accuracies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NBC</th>\n",
       "      <th>LDA</th>\n",
       "      <th>QDA</th>\n",
       "      <th>NSC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.936675</td>\n",
       "      <td>0.973615</td>\n",
       "      <td>0.970976</td>\n",
       "      <td>0.944591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.936842</td>\n",
       "      <td>0.963158</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.926316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            NBC       LDA       QDA       NSC\n",
       "Train  0.936675  0.973615  0.970976  0.944591\n",
       "Test   0.936842  0.963158  0.978947  0.926316"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Breast Cancer\n",
    "DISABLE_PLOTS = True\n",
    "VERBOSE = False\n",
    "DIM_RED = False\n",
    "QDA_HYPER = True\n",
    "NSC_HYPER = True\n",
    "#learn_dataset(data_file, None, 1, 1, \"M\", 2, features_fin = None, with_dim_red = False)\n",
    "learn_dataset(data_file = './data/wdbc.csv', header = None, random_state=1, \n",
    "              label_pos=1, label_value=\"M\", features_ini = 2, features_fin = None,\n",
    "              best_param_value_method=\"max_in_test\",\n",
    "              with_dim_red = DIM_RED, retained_variance = 99.0)\n",
    "\n",
    "learn_dataset(data_file = './data/wdbc.csv', header = None, random_state=1, \n",
    "              label_pos=1, label_value=\"M\", features_ini = 2, features_fin = None,\n",
    "              best_param_value_method=\"max_in_cv\",\n",
    "              with_dim_red = DIM_RED, retained_variance = 99.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prostate cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param value QuadraticDiscriminantAnalysis Method max_in_test: 0.3333333333333333\n",
      "Best param value NearestCentroid Method max_in_test: 1.6842105263157894\n",
      "\n",
      "Accuracies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NBC</th>\n",
       "      <th>LDA</th>\n",
       "      <th>QDA</th>\n",
       "      <th>NSC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.926471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            NBC       LDA       QDA       NSC\n",
       "Train  0.823529  0.823529  0.000000  0.926471\n",
       "Test   0.823529  0.852941  0.735294  0.941176"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best param value QuadraticDiscriminantAnalysis Method max_in_cv: 0.7777777777777777\n",
      "Best param value NearestCentroid Method max_in_cv: 2.526315789473684\n",
      "\n",
      "Accuracies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NBC</th>\n",
       "      <th>LDA</th>\n",
       "      <th>QDA</th>\n",
       "      <th>NSC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.911765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            NBC       LDA       QDA       NSC\n",
       "Train  0.823529  0.823529  0.000000  0.941176\n",
       "Test   0.823529  0.852941  0.647059  0.911765"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prostate Cancer\n",
    "#learn_dataset(data_file, 0, 1, -1, 1, 0, -1, with_dim_red = True)\n",
    "DISABLE_PLOTS = True\n",
    "VERBOSE = False\n",
    "DIM_RED = False\n",
    "QDA_HYPER = True\n",
    "NSC_HYPER = True\n",
    "learn_dataset(data_file = './data/prostate.csv', header = 0, random_state = 1, \n",
    "              label_pos = -1, label_value = 1, features_ini = 0, features_fin = -1,\n",
    "              best_param_value_method = \"max_in_test\",\n",
    "              with_dim_red = DIM_RED, retained_variance = 99.0)\n",
    "\n",
    "learn_dataset(data_file = './data/prostate.csv', header = 0, random_state = 1, \n",
    "              label_pos = -1, label_value = 1, features_ini = 0, features_fin = -1,\n",
    "              best_param_value_method = \"max_in_cv\",\n",
    "              with_dim_red = DIM_RED, retained_variance = 99.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We observe that **QDA** performs very poorly in the prostate dataset, given the high dimensionality of this dataset, which do not ease the accurate computation of the covariance matrices. Perhaps if we perform previously a dimensionality reduction by PCA, we'll improve this result.\n",
    "\n",
    "**NSC** performs in this case much better due to the reduced number of parameters and the feature selection properties of this classifier and more consistently between both cases (prostate and breast)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "latex_metadata": {
     "hidden": "true"
    }
   },
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "latex_metadata": {
     "hidden": "true",
     "lexer": "bash"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook classification_systems.ipynb to latex\n",
      "[NbConvertApp] Writing 46827 bytes to classification_systems.tex\n",
      "[NbConvertApp] Converting notebook classification_systems.ipynb to html_with_toclenvs\n",
      "[NbConvertApp] Writing 349054 bytes to classification_systems.html\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "jupyter nbconvert --to=latex --template=~/report.tplx classification_systems.ipynb 1> /dev/null\n",
    "pdflatex -shell-escape classification_systems 1> /dev/null\n",
    "jupyter nbconvert --to html_with_toclenvs classification_systems.ipynb 1> /dev/null"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "latex_metadata": {
   "author": "Daniel Cerdán, Fernando Freire",
   "title": "Classification Systems"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "sp"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "sp",
   "targetLang": "en",
   "useGoogleTranslate": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
