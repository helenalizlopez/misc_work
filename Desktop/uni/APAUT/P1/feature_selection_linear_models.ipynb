{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partially Supervised Feature Selection with Regularized Linear Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Feature selection methods overview\n",
    "\n",
    "This item is based con the first paper.\n",
    "\n",
    "**Goals of feature selection**\n",
    "\n",
    "Scenarios related to few tens of samples but thousands dimensions: microarray data, \n",
    "\n",
    "1. To avoid overfitting and improve model performance, prediction performance in the case of supervised classification and better cluster detection in unsupervised scenarios.\n",
    "\n",
    "2. To provide more efficient models\n",
    "\n",
    "3. To gain a deeper insight into the underlying processes that generated the data. The excess of dimensionality difficult the understanding.\n",
    "\n",
    "The problem is related to find the optimal model parameters for the optimal feature subset. So, the model parameters becomes dependent of the features selected and need to be computed more or less coupled with the guessing of model parameters.\n",
    "\n",
    "From less (zero) to more coupled computation, we have three strategies:\n",
    "\n",
    "1. Filter techniques. Two step process, firs the filtering, then the training of the model. Take into account only the properties of the data and in some cases a certain amount of prior knowledge. Therefore it's independent of the classification method. In its most simplest form ignores dependences on the data (univariate).\n",
    "\n",
    "    Examples: Euclidean distance, i-test Information gain, Markov blanket filter\n",
    "\n",
    "2. Wrapper methods. Once selected a candidate subset of features, the classification model is evaluated by training and testing the model. This is iterated over a ensemble of candidate subsets, and the model (with his feature subsets) selected is the model with the best accuracy. \n",
    "    \n",
    "    It's very important to construct a good searching algorithm of subsets, in order to reduce the number of sets to model with. This methods are dependent of the classifier, model feature dependencies and have the risk to be bind to a local optima. With randomizing techniques this problem is bypassed to some extent. \n",
    "    \n",
    "    Examples: Sequential forward selection (SFS) , Sequential backward elimination, Simulated annealing, Randomized hill climbing, Genetic algorithms.\n",
    "\n",
    "3. Embedded methods. The search of the optimal subset of features is built into the classifier. Have the advantage that they include the interaction with the classification model, while at the same time being far less computationally intensive than wrapper methods.\n",
    "\n",
    "    Examples: Decision trees Weighted naive Bayes, Feature selection using the weight vector of SVM, AROM\n",
    "    \n",
    "### AROM methods\n",
    "\n",
    "The acronym derives from *Approximation of Minimization zeRO-norm*\n",
    "\n",
    "The problem is obtain a linear predictor $h$, minimizing the number of independent variables (features) without loss of accuracy:\n",
    "\n",
    "$$h(\\mathbf{x}) = sign(\\mathbf{w} \\cdot \\mathbf{x} + b)$$\n",
    "\n",
    "for $n$ samples $x_i \\in \\mathbb{R}^n$ and $m$ labels $y_i \\in \\{\\pm1\\}$.\n",
    "\n",
    "The accuracy constraint requires correspondence of sign \n",
    "\n",
    "$sign(y_i) \\cdot sign(h_i) > 0$ or in other form $y_i \\cdot h_i = 1$\n",
    "\n",
    "or less restrictive, enabling $\\mathbf{w}$ to scale freely $y_i \\cdot h_i \\ge 1$\n",
    "\n",
    "so \n",
    "\n",
    "$$y_i(\\mathbf{w} \\cdot \\mathbf{x} + b) \\ge 1$$\n",
    "\n",
    "The minimization is done with a norm defined over the vectorial space of $\\mathbf{w}$. One approach is to minimize the zero-norm, that is, the number of components of the vector (number of non null $w_i$). But it's know to be a NP-Hard problem.\n",
    "\n",
    "It's more adequate compute over a 1-norm or a 2-norm. In the second paper, the author deduce a suitable form for the function that could be minimized, taken into account the former constraint:\n",
    "\n",
    "$$\\displaystyle\\sum_{j=1}^n ln(|w_j| + \\epsilon)$$\n",
    "\n",
    "The term $\\epsilon$ is included to protect from zero values inside logarithm.\n",
    "\n",
    "AROM methods are therefore feature selection embedded methods.\n",
    "\n",
    "**l1-AROM** and **l2-AROM** (in this case by means of a 2-norm minimization) algorithms optimize this algorithm by iterative rescaling of inputs and doing a smooth feature selection since the weight coefficients along some dimensions progressively drop below the machine precision while other dimensions become more significant.\n",
    "\n",
    "### AROM semi-supervised\n",
    "\n",
    "Third and Fourth papers explore a improvement of these previous described methods.\n",
    "\n",
    "**Goal**\n",
    "\n",
    "Classification of microarray data: few tens of samples against several thousand dimensions (genes).\n",
    "\n",
    "**Key differential strategy**\n",
    "\n",
    "Extend AROM methods by means of partial supervision on the dimensions of a feature selection procedure. The technique proposes to use of prior knowledge to guide feature selection, but flexible enough to let the final selection depart from it if necessary to optimize the classification objective.\n",
    "\n",
    "The preferential features are previously selected from similar datasets in large microarray databases because it's known that different sub-samples of patients lead to very similar sets of biomarkers, as expected if we are aware that the biological process explaining the outcome is common among different patients.\n",
    "\n",
    "This datasets are called source datasets and we expect that the prediction for a similar feature vector is the same than the prediction for this vector in our dataset (the target).\n",
    "\n",
    "*In third paper prior knowledge is incorporated by biological information*\n",
    "\n",
    "So, if we have some knowledge on the relative importance of each feature (either from actual prior knowledge or from a related dataset), the supervised AROM objective can be modified by adding a prior relevance vector $\\beta = [\\beta_1,...,\\beta_n]$  defined over the $n$ dimensions and where $\\beta_j >0$ is the prior relevance of the $j$ feature.\n",
    "\n",
    "So in this case, the function to minimize in the case of 1-norm is:\n",
    "\n",
    "$$\\displaystyle\\sum_{j=1}^n \\frac{1}{\\beta_j} ln(|w_j| + \\epsilon)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project planification\n",
    "\n",
    "The scheduling for the first scrum sprint (and the unique sprint) is showed in the figure.\n",
    "\n",
    "![Roadmap](planing.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "latex_metadata": {
     "hidden": "true"
    }
   },
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "latex_metadata": {
     "hidden": "true",
     "lexer": "bash"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook feature_selection_linear_models.ipynb to latex\n",
      "[NbConvertApp] Writing 32105 bytes to feature_selection_linear_models.tex\n",
      "[NbConvertApp] Converting notebook feature_selection_linear_models.ipynb to html_with_toclenvs\n",
      "[NbConvertApp] Writing 285913 bytes to feature_selection_linear_models.html\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "jupyter nbconvert --to=latex --template=~/report.tplx feature_selection_linear_models.ipynb 1> /dev/null\n",
    "pdflatex -shell-escape feature_selection_linear_models 1> /dev/null\n",
    "jupyter nbconvert --to html_with_toclenvs feature_selection_linear_models.ipynb 1> /dev/null"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "latex_metadata": {
   "author": "Daniel Cerd√°n, Fernando Freire",
   "title": "Partially Supervised Feature Selection with Regularized Linear Models"
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
